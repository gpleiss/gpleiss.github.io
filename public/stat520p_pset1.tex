%%%%% Custom comments stuff
\newif\ifcomments
%\commentsfalse
 \commentstrue

\documentclass[11pt,letterpaper]{article}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{amsmath,amsfonts,amsthm,amssymb,cancel}  % Standard math packages
\usepackage{hyperref}  % For hyperlinks
\usepackage[capitalize]{cleveref}  % for better citations
\usepackage{color,xcolor}  % For colored text
\usepackage{enumitem}  % For better list support
\usepackage{titlesec}  % For formatting title
\usepackage{comment}  % For ``commenting'' out answer envirnoment


% Edit title
\makeatletter
\renewcommand{\@maketitle}{
  \begin{center}%
    {\Large \bf \noindent \textsc{\@title}}%
    \\
    {\large \noindent \textsc{\@author}}%
    \vspace{-0.5em}
  \end{center}%
}
\makeatother


% Redefine section headers/paragraph spacing
\titleformat{\section}{\large\bfseries\scshape}{\thesection)}{1em}{}
\titlespacing{\section}{0em}{1em}{-0.25em}
\titlespacing{\paragraph}{0em}{0em}{0.5em}
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

% Section break
\newcommand{\sectionbreak}{%
  \vspace{1em}
  \hrule
}


% Macros
\input{math_commands}


%% Document info
\title{STAT520P Diagnostic Problem Set}
\author{Geoff Pleiss}  % Please replace with your name!


% Turn answers on and off
\newif\ifshowanswers
%\showanswersfalse
\showanswerstrue

% Change document color
\ifshowanswers
  \newcommand{\doccolor}{gray}
\else
  \newcommand{\doccolor}{black}
\fi

% Answer environment
\ifshowanswers
  \newenvironment{answer}{%
    \vspace{1em}
    \color{black}
    \paragraph{Answer.}
  }{%
    \vspace{1em}
  }
\else
  \excludecomment{answer}
\fi


%% Document
\begin{document}
\maketitle

\color{\doccolor}
The purpose of this problem set is to ensure that you feel comfortable with multivariate Gaussian distributions and their manipulations.
(They come up a lot in Bayesian optimization.)

If you have a strong background in Bayesian statistics, this problem set should be fairly straightforward.
(Hopefully you will learn a new derivation or two!)
If you are new to Gaussian distributions, this problem set should build fluency that you will need for the class.
If these problems feels extremely difficult, then you will likely find this course to be technically overwhelming.

\sectionbreak

\paragraph{A quick note on notation.}
Variable names should use the following convention:
\begin{itemize}
  \item deterministic scalars will be represented by lowercase/non-bold letters (e.g. $a$, $\theta$, etc.);
  \item deterministic vectors will be represented by lowercase/bold letters (e.g. $\va$, $\vtheta$, etc.);
  \item deterministic matrices will be represented by uppercase/bold letters (e.g. $\mA$, $\mTheta$, etc.); and
  \item all random variables---scalar, vector, or matrix---will be represented by uppercase/non-bold letters (e.g. $A$, $\Theta$, etc.).
\end{itemize}
%
(For the rest of the course, we will often use the same notation for deterministic and random variables.
However, I am differentiating them in this problem set for clarity.)

$p(Y=\va)$ refers to the density of the random variable $Y$ evaluated at $\va$.
$\normal(a; \mu, \sigma^2)$ refers to the function that evaluates the $\mu$-mean $\sigma^2$-variance Gaussian density on the scalar $a \in \R$;
i.e.
\begin{equation}
  \normal \left( a; \mu, \sigma^2 \right)
  = (2 \pi \sigma^2)^{-1/2} \exp \left( -\tfrac{1}{2 \sigma^2} (a - \mu)^2 \right).
  \label{eqn:univariate_density}
\end{equation}
With a slight abuse of notation,
$Y \sim \normal(\mu, \sigma^2)$ should be read as ``the random variable $Y$ is Gaussian distributed with mean $\mu$ and variance $\sigma^2$''---%
i.e. $p(Y = a) = \normal(a; \mu, \sigma^2).$
Analogous notation will be used for multivariate Gaussian distributions (but you will first have to derive the density!).

\sectionbreak

In this problem set, you will deriving properties of Gaussian distributions from first principles.
You should solve all of these problems using only the following rules:
\begin{enumerate}
  \item the sum rule---$p(Y=\va) = \int p(Y=\va, Z=\vb) \intd \vb$;
  \item the product rule---$p(Y\!=\!\va, Z\!=\!\vb) = p(Y\!=\!\va \mid Z\!=\!\vb) p(Z\!=\!\vb) = p(Z\!=\!\vb \mid Y\!=\!\va) p(Y\!=\!\va)$;
    with $p(Y\!=\!\va, Z\!=\!\vb) = p(Y\!=\!\va) p(Z\!=\!\vb)$ if and only if $Y$ and $Z$ are independent;
  \item the change of variables formula---if $\vg(\cdot)$ is a differentiable and bijective function, then
    $$p(Y\!=\!\va) = \det \left( \mJ_{\vg}(\va) \right) \: p(\vg(Y)\!=\!\vg(\va)),$$
    where $\mJ_\vg(\va)$ is the Jacobian matrix of $\vg$ evaluated at $\va$;
  \item linearity of expectation---$\E[ \mA Y + \mB Z + \vc ] = \mA \E[Y] + \mB \E[Z] + \vc$;
  \item the \emph{univariate} Gaussian density (Eq.~\ref{eqn:univariate_density}); and
  \item any linear algebraic identities that you want.
\end{enumerate}


\clearpage

\section{(10 pts) The Univariate Linear Gaussian Identity}

Consider the univariate Gaussian random variable $Y \sim \normal(\mu, \sigma^2)$.

\begin{enumerate}
  \item (3 pts) Since $\normal(a; \mu, \sigma^2)$ is a density, we have that
    \begin{equation}
      \int_{-\infty}^\infty \frac{1}{(2 \pi \sigma^2)^{1/2}} e^{-\tfrac{1}{2 \sigma^2} (a - \mu)^2} \intd a = 1.
      \label{eqn:univariate_density_int}
    \end{equation}
    Prove that $\E[Y - \mu] = 0$ and $\E[(Y - \mu)^2] = \sigma^2$ by differentiating both sides of \cref{eqn:univariate_density_int}.

\begin{answer}
  TODO
\end{answer}

  \item (2 pts) Let $Y, Y' \sim \normal(0, 1)$ be two i.i.d. standard Gaussian random variables.
    Write out the joint density $p(Y\!=\!(b-a), Y'\!=\!a)$ and simplify.

\begin{answer}
  TODO
\end{answer}

  \item (3 pts) Using your answer above, prove that
    $\int_{-\infty}^{\infty} p(Y\!=\!(b-a)) p(Y'\!=\!a) \intd a = (4\pi)^{-1/2} \exp(-\tfrac{1}{2^2} b^2).$
    (Hint: you should be able to prove this in ~4 lines by \href{https://en.wikipedia.org/wiki/Completing_the_square}{completing the square}
    and using \cref{eqn:univariate_density_int}.)

\begin{answer}
  TODO
\end{answer}

  \item (2 pts) Based on the previous result, what can you say about the distribution of the random variable $Z = Y + Y'$?

\begin{answer}
  TODO
\end{answer}

\end{enumerate}

The previous result is a special case of the \emph{linear Gaussian identity}, which is arguably the most powerful property of Gaussian distributions.
More generally, if $Y$ and $Y'$ are independent Gaussian random variables with
$Y \sim \normal(\mu, \sigma^2)$ and $Y' \sim \normal(\mu', \sigma^{\prime2})$, then for any $a, b, c \in R$, we have
%
\begin{align}
  (aY + bY' + c) \sim \normal\left(
    a\mu + b\mu' + c,
    \:\:
    a^2 \sigma^2 + b^2 \sigma^{\prime 2}
  \right).
  \label{eqn:univariate_sum}
\end{align}
%
You can prove this result with the same techniques as above, but it requires more bookkeeping.


\section{(25 pts) Multivariate Gaussian Random Variables}

\paragraph{Definition:}
Let $Y$ be a $d$-dimensional vector-valued random variable.
$Y$ is \emph{multivariate Gaussian} if and only all linear combination of its entries are univariate Gaussian;
i.e. for all $\vc \in \R^d$, we have that $\vc^\top Y \sim \normal(\mu, \sigma^2)$ for some $\mu, \sigma \in \R$.

\begin{enumerate}
  \item (2 pts) Let $U = \begin{bmatrix} U_1 & \ldots & U_d\end{bmatrix}$ be a random $d$-dimensional vector,
    where $U_1$, $\ldots$, $U_d$ are all i.i.d. standard Gaussian random variables.
    ($U_1, \ldots, U_d \simiid \normal(0, 1).$)
    Prove that $U$ meets the definition of a multivariate Gaussian random variable.

\begin{answer}
  TODO
\end{answer}

  \item (3 pts) Consider the random vector $Y = \mL U + \vmu$, where $\vmu$ and $\mL$ are deterministic.
    Prove that $Y$ also meets the definition for a multivariate Gaussian random variable, and compute its mean and covariance.

\begin{answer}
  TODO
\end{answer}

  \item (4 pts) Let $Z$ be a multivariate Gaussian random variable where $\E[Z] = \vmu$ and \\
    $\E[(Z - \E[Z])(Z - \E[Z])^\top] = \mL\mL^\top$.
    Prove that, for any $a \in R$ and $c \in \R^d$, we have that \\
    $p((\vc^\top Z)\!=\!a) \: = \: p((\vc^\top (\mL^\top U + \vmu))\!=\!a)$.
    (Hint: use the fact that the density of a univariate normal distribution is determined by its mean and variance.)

\begin{answer}
  TODO
\end{answer}

\end{enumerate}

The last fact, taken together with the \href{https://en.wikipedia.org/wiki/Cram\%C3\%A9r\%E2\%80\%93Wold_theorem}{Cram\'er-Wold theorem},
implies that $p(Z \! = \! \va) = p((\mL U + \mu) \! = \! \va)$ for all $\va \in \R^d$.
In other words, \emph{two multivariate Gaussian random variables are equal in distribution if they share the same mean and covariance}.
We will exploit this fact to derive a density for $Z$.


\begin{enumerate}[resume]
  \item (2 pts) Write the joint density $p(U = \va)$ as a matrix.

\begin{answer}
  TODO
\end{answer}


  \item (4 pts) Assume that $\mL$ is a square matrix, and define $\mK = \mL\mL^\top$.
    Using the change-of-variables formula, prove that the density of $\mL U + \vmu$ is
    \begin{equation}
      \normal \left( \va; \vmu, \mK \right)
      := \tfrac{1}{\vert 2 \pi \mK \vert^{1/2}} \exp \left( -\tfrac{1}{2} (\va - \vmu)^\top \mK^{-1} (\va - \vmu) \right).
      \label{eqn:mvn_density}
    \end{equation}

\begin{answer}
  TODO
\end{answer}

\end{enumerate}

These last two results demonstrate that if $Z$ is multivariate Gaussian with mean $\mu$ and covariance $\mK$,
then the density of $Z$ is given by \cref{eqn:mvn_density}.
Moreover, we have also demonstrated that $\mK = \mL \mL^\top$, and therefore the covariance must be positive semi-definite.

\begin{enumerate}[resume]
  \item
    (5 pts)
    Consider the following multivariate Gaussian, written in block matrix form:
    %
    \begin{equation*}
      \begin{bmatrix} Y \\ Y^\prime \end{bmatrix}
      \sim
      \normal \left(
        \begin{bmatrix}
          \vzero \\ \vzero
        \end{bmatrix},
        \begin{bmatrix}
          \mK & \mK^\prime \\
          \mK^{\prime\top} & \mK^{\prime\prime}
        \end{bmatrix}
      \right),
    \end{equation*}
    %
    where $Y$ is $d$-dimensional and $Y'$ is $d'$-dimensional.
    Prove that if $\mK' = \vzero$ then $Y$ and $Y'$ are independent Gaussian random variables.

\begin{answer}
  TODO
\end{answer}

  \item (5 pts) Prove the following generalization of the linear Gaussian identity:
    if $Y \sim \normal(\vmu, \mL\mL^\top)$ and $Y' \sim \normal(\vmu', \mL' \mL^{\prime\top})$ are independent multivariate Gaussian random variables, then
    \begin{equation}
      p\left( \mA Y + \mB Y' + \vc \right) \sim \normal(\mA \vmu + \mB \vmu' + \vc, \: \mA \mL\mL^\top \mA^\top + \mB \mL'\mL^{\prime\top} \mB^\top).
      \label{eqn:affine_density}
    \end{equation}
    (Hint: you can prove this in 3-5 lines using the previous results and some clever linear algebra.)

\begin{answer}
  TODO
\end{answer}

\end{enumerate}



\section{(15 pts) Marginal and Conditional Distributions}
\emph{Using results from the previous problems, answers to these sub-problems should each be about 1-5 lines long!}

Consider the following multivariate Gaussian, written in block matrix form:
%
\begin{equation*}
  \begin{bmatrix} Y \\ Y^\prime \end{bmatrix}
  \sim
  \normal \left(
    \begin{bmatrix}
      \vzero \\ \vzero
    \end{bmatrix},
    \begin{bmatrix}
      \mK & \mK^\prime \\
      \mK^{\prime\top} & \mK^{\prime\prime}
    \end{bmatrix}
  \right),
\end{equation*}
%
where $Y$ is $d$-dimensional and $Y^\prime$ is $d^\prime$-dimensional.
%
\begin{enumerate}
  \item (5 pts) Without performing any integration, prove that the marginal density of $Y$ is equal to
  \begin{equation}
    p(Y = \va)
    = \int p\left( \begin{bmatrix} Y \\ Y' \end{bmatrix} = \begin{bmatrix} \va \\ \va' \end{bmatrix} \right) \intd \va'
    = \normal(\va; \vmu, \mK).
    \label{eqn:marginalization}
  \end{equation}

\begin{answer}
  TODO
\end{answer}

  \item (5 pts) Define the random variable $Z$ such that
    $$
      \begin{bmatrix}
        Y \\ Z
      \end{bmatrix}
      =
      \begin{bmatrix}
        \mI & \vzero \\
        -\mK^{\prime\top}\mK^{-1} & \mI
      \end{bmatrix}
      \begin{bmatrix} Y \\ Y' \end{bmatrix}.
    $$
    Prove that $Y$ and $Z$ are independent, and derive the distribution of $Z$.
    (If the matrix on the right hand side seems arbitrary for you, then remind yourself about \href{https://en.wikipedia.org/wiki/Gaussian_elimination}{Gaussian elimination}.)

\begin{answer}
  TODO
\end{answer}

  \item (5 pts) Combine the previous two results to show that
    \begin{equation}
      p\left( Y' = \va' \mid Y = \va \right) = \normal \left(
        \va'; \:\: \mK^{\prime\top} \mK^{-1} \va, \: \mK^{\prime\prime} - \mK^{\prime\top} \mK^{-1} \mK
      \right).
      \label{eqn:conditioning}
    \end{equation}
    (Hint: use the product rule, and the fact that $Z$ is determined by $Y$ and $Y'$.)

\begin{answer}
  TODO
\end{answer}

\end{enumerate}



\sectionbreak

From the previous results, we have proven the following (remarkable) facts about multivariate Gaussian random variables:
%
\begin{enumerate}
  \item any multivariate Gaussian random variable is a rotation/shift of independent Gaussian random variables,
  \item affine transformations and linear combinations of Gaussians are Gaussian (Eq.~\ref{eqn:affine_density}),
  \item multivariate Gaussian random variables are closed under marginalization (Eq.~\ref{eqn:marginalization}), and
  \item multivariate Gaussian conditionals are Gaussian (Eq.~\ref{eqn:conditioning}).
\end{enumerate}


\end{document}
